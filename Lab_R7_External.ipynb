{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_R7_External.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4WH1Pr4KQlCh","colab_type":"text"},"source":["### Build a DNN using Keras with `RELU` and `ADAM`"]},{"cell_type":"markdown","metadata":{"id":"TbvI8LqlQlCl","colab_type":"text"},"source":["#### Load tensorflow"]},{"cell_type":"code","metadata":{"id":"SPW-a-qYQlCp","colab_type":"code","outputId":"98a12a79-bc78-41a3-db66-d271818b2993","executionInfo":{"status":"ok","timestamp":1578728540086,"user_tz":-330,"elapsed":1396,"user":{"displayName":"anish khadgi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLC1ZU-4V6glW4RhXxNsD9B2FY4dsVA9NVh_-unA=s64","userId":"00711144572319287653"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["2.1.0-rc1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MiXZmX2Zf1kE","colab_type":"text"},"source":["Importing tensorflow and setting version as 2.0 above"]},{"cell_type":"code","metadata":{"id":"El31A9rYBYGu","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74cQBsi5QlCw","colab_type":"text"},"source":["#### Collect Fashion mnist data from tf.keras.datasets "]},{"cell_type":"code","metadata":{"id":"wVWy0oDTr2Kj","colab_type":"code","colab":{}},"source":["from keras.datasets import fashion_mnist\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bqmzePuDdGG","colab_type":"code","outputId":"e1850e58-19de-402a-878a-e23ac0ceeae4","executionInfo":{"status":"ok","timestamp":1578728553865,"user_tz":-330,"elapsed":1351,"user":{"displayName":"anish khadgi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLC1ZU-4V6glW4RhXxNsD9B2FY4dsVA9NVh_-unA=s64","userId":"00711144572319287653"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["print('train dataset shape:')\n","print(x_train.shape)\n","print('test dataset shape:')\n","print(x_test.shape)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["train dataset shape:\n","(60000, 28, 28)\n","test dataset shape:\n","(10000, 28, 28)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"no7aWYZyQlC1","colab_type":"text"},"source":["#### Change train and test labels into one-hot vectors"]},{"cell_type":"code","metadata":{"id":"UX6otc4wQlC2","colab_type":"code","colab":{}},"source":["y_train_cat = tf.keras.utils.to_categorical(y_train)\n","y_test_cat = tf.keras.utils.to_categorical(y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zs1PAn4blUbk","colab_type":"text"},"source":["As this is multi-class problem, categorical crossentropy will be used as a loss function which requires Labels in the data to be one-hot encoded."]},{"cell_type":"markdown","metadata":{"id":"QjNrRTdoQlC5","colab_type":"text"},"source":["#### Build the Graph"]},{"cell_type":"markdown","metadata":{"id":"CDJ9DHVNQlC7","colab_type":"text"},"source":["#### Initialize model, reshape & normalize data"]},{"cell_type":"code","metadata":{"id":"GAUu0yELBo-P","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.layers import Reshape\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import BatchNormalization"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCDQs_g1QlC8","colab_type":"code","colab":{}},"source":["clear_session()\n","\n","#Initialize model\n","model1 = Sequential()\n","\n","#Reshape data from 2D to 1D -> 28x28 to 784\n","model1.add(Reshape((784,),input_shape=(28,28,)))\n","\n","#Normalize the data\n","model1.add(BatchNormalization())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBGwTTilQlDD","colab_type":"text"},"source":["#### Add two fully connected layers with 200 and 100 neurons respectively with `relu` activations. Add a dropout layer with `p=0.25`"]},{"cell_type":"code","metadata":{"id":"IXbfpfOzQlDF","colab_type":"code","colab":{}},"source":["#Add First Dense layer\n","model1.add(Dense(200, activation='relu'))\n","\n","#Add Second Dense layer\n","model1.add(Dense(100, activation='relu'))\n","\n","#Add dropout layer\n","model1.add(Dropout(0.25))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5I8f5otcQlDJ","colab_type":"text"},"source":["### Add the output layer with a fully connected layer with 10 neurons with `softmax` activation. Use `categorical_crossentropy` loss and `adam` optimizer and train the network. And, report the final validation."]},{"cell_type":"code","metadata":{"id":"JZkvKymSd0Sr","colab_type":"code","colab":{}},"source":["#Add Output layer\n","model1.add(Dense(10, activation='softmax'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNGJH437DHGU","colab_type":"code","colab":{}},"source":["#Compile model with categorical_crossentropy loss and adam optimizer\n","model1.compile(optimizer='adam', \n","              loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"54cIBQ_rDKsB","colab_type":"code","outputId":"79d9c581-4fce-4ee8-9e29-8fa3be39566a","executionInfo":{"status":"ok","timestamp":1578728582048,"user_tz":-330,"elapsed":1354,"user":{"displayName":"anish khadgi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLC1ZU-4V6glW4RhXxNsD9B2FY4dsVA9NVh_-unA=s64","userId":"00711144572319287653"}},"colab":{"base_uri":"https://localhost:8080/","height":386}},"source":["model1.summary()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","reshape (Reshape)            (None, 784)               0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 784)               3136      \n","_________________________________________________________________\n","dense (Dense)                (None, 200)               157000    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 100)               20100     \n","_________________________________________________________________\n","dropout (Dropout)            (None, 100)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                1010      \n","=================================================================\n","Total params: 181,246\n","Trainable params: 179,678\n","Non-trainable params: 1,568\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V9xqkAd1FLYw","colab_type":"text"},"source":["Model summary above depicts the model representation.\n","\n","Reshaping layer is the first layer or input layer which does not have any parameters (as reshaped layer output is directly fed to the subsequent dense layer with is first hidden layer).\n","\n","Batch Normalization layer in this model normalizes the previous layer output at each batch.This leads to 784 * 2 = 1568 non-trainable parameters mu and signma i.e. not updated by gradient descent and trainable parameters gamma and beta which are updated based on required scaling and shifting as per gradient descent.Batchnormalization layer is used to reduce overfitting and also normalization helps reduce the effect of varying distribution of input data when test data is fed to the model.\n","\n","After Batch Normalization layer this model has two fully connected or dense hidden layers with given number of neurons.\n","\n","Dropout layer in this model temporarily removes 25% of the neurons randomly in the second hidden layer.This layer helps to reduce overfitting.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ddf79hIuDPuv","colab_type":"code","outputId":"5b1b18fc-1ccf-4787-a63b-81abc5e62f21","executionInfo":{"status":"ok","timestamp":1578728639518,"user_tz":-330,"elapsed":51648,"user":{"displayName":"anish khadgi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLC1ZU-4V6glW4RhXxNsD9B2FY4dsVA9NVh_-unA=s64","userId":"00711144572319287653"}},"colab":{"base_uri":"https://localhost:8080/","height":239}},"source":["#Train the model\n","model1.fit(x_train,y_train_cat,          \n","          validation_data=(x_test,y_test_cat),\n","          epochs=5,\n","          batch_size=32)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/5\n","60000/60000 [==============================] - 11s 177us/sample - loss: 0.5123 - accuracy: 0.8172 - val_loss: 0.4039 - val_accuracy: 0.8496\n","Epoch 2/5\n","60000/60000 [==============================] - 10s 166us/sample - loss: 0.3965 - accuracy: 0.8564 - val_loss: 0.4030 - val_accuracy: 0.8534\n","Epoch 3/5\n","60000/60000 [==============================] - 10s 165us/sample - loss: 0.3616 - accuracy: 0.8688 - val_loss: 0.3724 - val_accuracy: 0.8644\n","Epoch 4/5\n","60000/60000 [==============================] - 10s 164us/sample - loss: 0.3358 - accuracy: 0.8763 - val_loss: 0.3558 - val_accuracy: 0.8743\n","Epoch 5/5\n","60000/60000 [==============================] - 10s 162us/sample - loss: 0.3188 - accuracy: 0.8811 - val_loss: 0.3599 - val_accuracy: 0.8705\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fa200137d30>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"fm5LPRjtiezg","colab_type":"text"},"source":["Model was trained with 5 epochs which means all training examples are fed to the model 5 times and batch size of 32 which means weights are updated after processing every batch of 32 training examples.\n","\n"]},{"cell_type":"code","metadata":{"id":"Ft5dm-4mIagX","colab_type":"code","outputId":"27686c28-63d7-4495-deeb-ff5ffbb957ef","executionInfo":{"status":"ok","timestamp":1578728645556,"user_tz":-330,"elapsed":1523,"user":{"displayName":"anish khadgi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDLC1ZU-4V6glW4RhXxNsD9B2FY4dsVA9NVh_-unA=s64","userId":"00711144572319287653"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print('First Model Evaluation on test data: ')\n","results = model1.evaluate(x_test, y_test_cat, verbose=0)\n","print('Test loss, Test Accuracy : ', results)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["First Model Evaluation on test data: \n","Test loss, Test Accuracy :  [0.3599478364944458, 0.8705]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fto-Py_qj5HZ","colab_type":"text"},"source":["Model training with 5 epochs and batch size of 32 gives the test accuracy of around 87% and loss (categorical crossentropy or multi-class log loss) value of around 0.359 "]}]}